Interesting paper collection

We all know that kind of feeling when you discover some very interesting paper but you never really have time to read it. Maybe you just throw it to Zotero and it stays there forever. Pobably posting the interesting/useful papers here will give me some more motivation to revisit them later? Let's try.

**1.29.2024**

[Neural tuning and representational geometry](https://www.nature.com/articles/s41583-021-00502-3), *Nature Reviews Neuroscience*, 2021
Nikolaus Kriegeskorte & Xue-Xin Wei

**1.30.2024**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9), *Nature Machine Intelligence*, 2023
Jascha Achterberg et al.

**2.1.2024**

[Transformer as a hippocampal memory consolidation
model based on NMDAR-inspired nonlinearity](https://openreview.net/pdf?id=vKpVJxplmB), *NeurIPS*, 2023

**2.5.2024**

[Brains and algorithms partially converge in natural language processing](https://www.nature.com/articles/s42003-022-03036-1), *Communications Biology*, 2022

**2.7.2024**

[No Coincidence, George: Capacity-Limits as the Curse of Compositionality](https://osf.io/preprints/psyarxiv/cjuxb), PsyArXiv, 2022

**2.12.2024**

[Structural constraints on the emergence of oscillations in multi-population neural networks](https://elifesciences.org/reviewed-preprints/88777), eLife, 2024

[Oscillatory neural networks](https://www.youtube.com/watch?v=RNr7-gHxB4g), YouTube

**2.14**

[Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons](https://link.springer.com/article/10.1023/a:1008925309027)

**2.16**

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

[Mechanisms of Gamma Oscillations](https://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150444)

**2.17**

[A call for embodied AI](https://arxiv.org/abs/2402.03824)

**2.18**

[Circular and unified analysis in network neuroscience](https://elifesciences.org/articles/79559)

**2.20-2.27**

I was at AAAI 2024 for nearly a week. I learned a lot and will share some papers I came across from talks/posters at the conference.

[On the Paradox of Learning to Reason from Data](https://arxiv.org/pdf/2205.11502.pdf)

[CRAB: Assessing the Strength of Causal Relationships
Between Real-World Events](https://arxiv.org/pdf/2311.04284.pdf)

[Passive learning of active causal strategies in agents
and language models](https://arxiv.org/pdf/2305.16183.pdf)

[SPARTQA: A Textual Question Answering Benchmark
for Spatial Reasoning](https://arxiv.org/pdf/2104.05832.pdf)

[Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

**3.1**

[Three aspects of representation in neuroscience](https://www.sciencedirect.com/science/article/pii/S1364661322002108)

[Redefining &#34;Hallucination&#34; in LLMs: Towards a psychology-informed framework for mitigating misinformation](https://arxiv.org/abs/2402.01769v1)

[Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

**3.2**

[Neural Turing Machines](https://arxiv.org/abs/1410.5401)

[A Critical Review of Causal Reasoning Benchmarks for Large Language Models](https://openreview.net/pdf?id=mRwgczYZFJ)

**3.3**

[Recurrent Models of Visual Attention](https://arxiv.org/pdf/1406.6247.pdf)

[Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762)

[Multiple Object Recognition with Visual Attention](https://arxiv.org/abs/1412.7755)

[Attention is not all you need anymore](https://arxiv.org/pdf/2308.07661.pdf)

[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=AIiwuClvH6k)

**3.7**

[Large language models surpass human experts in
predicting neuroscience results](https://arxiv.org/pdf/2403.03230.pdf)

**3.8**

[Encoding and decoding in fMRI](https://doi.org/10.1016/j.neuroimage.2010.07.073)

[My favorite math jokes](https://arxiv.org/pdf/2403.01010v1.pdf)

**3.9**

[Memory in humans and deep language models: Linking hypotheses for model augmentation](https://arxiv.org/abs2210.1869)

**3.11**

[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)

[Mathematical introduction to deep learning](https://arxiv.org/pdf/2310.20360.pdf)

**3.12**

[Memory and attention in deep learning](https://arxiv.org/abs/2107.01390)

[World Models and Predictive Coding for Cognitive and Developmental Robotics:
Frontiers and Challenges](https://arxiv.org/pdf/2301.05832.pdf)

[Mastering Memory Tasks with World Models](https://arxiv.org/pdf/2403.04253v1.pdf)

[Mechanism for feature learning in neural networks and backpropagation-free machine learning models](https://www.science.org/doi/10.1126/science.adi5639)

**3.13**

[Brain-inspired intelligent robotics: The intersection of robotics and neuroscience](https://www.science.org/do/10.1126/resource.2375862/full/brain_inspired_robotics_booklet_2016_12_16-1686067069503.pdf)

[Papers mentioned in this article](https://www.jiqizhixin.com/articles/2016-01-04-4)

**3.14**

[One model for the learning of language](https://www.pnas.org/doi/full/10.1073/pnas.2021865119)

**3.15**

[The pitfalls of next-token prediction](https://arxiv.org/pdf/2403.06963.pdf)

**3.16**

[Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

**3.18**

[Neuroscience needs behavior](https://www.cell.com/neuron/pdf/S0896-6273%2816%2931040-6.pdf)

**3.23**

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1)

[Task interference as a neuronal basis for the cost of cognitive flexibility](https://www.biorxiv.org/content/10.1101/2024.03.04.583375v1.full.pdf)

[A Technical Critique of Some Parts of the Free Energy Principle](https://arxiv.org/abs/2001.06408)

**3.24**

[Theories of Error Back-Propagation in the Brain](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9)

[Neurosymbolic AI](https://arxiv.org/abs/2305.00813)

**3.26**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9)

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1.full)

**3.27**

[Reconstructing computational system dynamics from neural data with recurrent neural networks](https://www.nature.com/articles/s41583-023-00740-7)

**3.29**

[A useful guide of how to pronounce common math symbols](http://www.uefap.com/speaking/symbols/symbols.htm)

**3.30**

[A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/pdf/2403.18929.pdf)

**3.31**

[Collective intelligence: A unifying concept for integrating biology across scales and substrates](https://www.nature.com/articles/s42003-024-06037-4)

**4.3**

[An Introduction to Model-Based Cognitive Neuroscience](https://link.springer.com/book/10.1007/978-3-031-45271-0)

[What does it mean to understand a neural network?](https://arxiv.org/abs/1907.06374)

[What is a GPT by 3Blue1Brown](https://www.youtube.com/watch?v=wjZofJX0v4M)

**4.5**

[Nonmonotonic Plasticity: How Memory Retrieval Drives Learning](https://www.sciencedirect.com/science/article/pii/S1364661319301597)

[Single Cortical Neurons as Deep Artificial Neural Networks](https://www.sciencedirect.com/science/article/abs/pii/S0896627321005018)

**4.17**

[The brain's unique take on algorithms](https://www.nature.com/articles/s41467-023-40535-z)

[Cognition is an emergent property](https://www.sciencedirect.com/science/article/pii/S2352154624000391?via%3Dihub)

**4.18**

[Catalyzing next-generation Artificial Intelligence through NeuroAI](https://www.nature.com/articles/s41467-023-37180-x)

**4.19**

[Toward a formal theory for computing machines made out of whatever physics offers](https://www.nature.com/articles/s41467-023-40533-1)

[Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research](https://www.sciencedirect.com/science/article/pii/S0893608021003683)

**4.22**

[Time, Love, Memory](https://en.wikipedia.org/wiki/Time,_Love,_Memory)
[Thinking About Science](https://www.science.org/doi/10.1126/science.242.4886.1711)
