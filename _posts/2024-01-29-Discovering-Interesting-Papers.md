Interesting NeuroAI/LLM Cognition/Emobodied AI/miscellaneous papers

We all know that kind of feeling when you discover some very interesting paper but you never really have time to read it. Maybe you just throw it to Zotero and it stays there forever. Pobably posting the interesting/useful papers here will give me some more motivation to revisit them later? Let's try.

**1.29.2024**

[Neural tuning and representational geometry](https://www.nature.com/articles/s41583-021-00502-3), *Nature Reviews Neuroscience*, 2021
Nikolaus Kriegeskorte & Xue-Xin Wei

**1.30.2024**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9), *Nature Machine Intelligence*, 2023
Jascha Achterberg et al.

**2.1.2024**

[Transformer as a hippocampal memory consolidation
model based on NMDAR-inspired nonlinearity](https://openreview.net/pdf?id=vKpVJxplmB), *NeurIPS*, 2023

**2.5.2024**

[Brains and algorithms partially converge in natural language processing](https://www.nature.com/articles/s42003-022-03036-1), *Communications Biology*, 2022

**2.7.2024**

[No Coincidence, George: Capacity-Limits as the Curse of Compositionality](https://osf.io/preprints/psyarxiv/cjuxb), PsyArXiv, 2022

**2.12.2024**

[Structural constraints on the emergence of oscillations in multi-population neural networks](https://elifesciences.org/reviewed-preprints/88777), eLife, 2024

[Oscillatory neural networks](https://www.youtube.com/watch?v=RNr7-gHxB4g), YouTube

**2.14**

[Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons](https://link.springer.com/article/10.1023/a:1008925309027)

**2.16**

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

[Mechanisms of Gamma Oscillations](https://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150444)

**2.17**

[A call for embodied AI](https://arxiv.org/abs/2402.03824)

**2.18**

[Circular and unified analysis in network neuroscience](https://elifesciences.org/articles/79559)

**2.20-2.27**

I was at AAAI 2024 for nearly a week. I learned a lot and will share some papers I came across from talks/posters at the conference.

[On the Paradox of Learning to Reason from Data](https://arxiv.org/pdf/2205.11502.pdf)

[CRAB: Assessing the Strength of Causal Relationships
Between Real-World Events](https://arxiv.org/pdf/2311.04284.pdf)

[Passive learning of active causal strategies in agents
and language models](https://arxiv.org/pdf/2305.16183.pdf)

[SPARTQA: A Textual Question Answering Benchmark
for Spatial Reasoning](https://arxiv.org/pdf/2104.05832.pdf)

[Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

**3.1**

[Three aspects of representation in neuroscience](https://www.sciencedirect.com/science/article/pii/S1364661322002108)

[Redefining &#34;Hallucination&#34; in LLMs: Towards a psychology-informed framework for mitigating misinformation](https://arxiv.org/abs/2402.01769v1)

[Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

**3.2**

[Neural Turing Machines](https://arxiv.org/abs/1410.5401)

[A Critical Review of Causal Reasoning Benchmarks for Large Language Models](https://openreview.net/pdf?id=mRwgczYZFJ)

**3.3**

[Recurrent Models of Visual Attention](https://arxiv.org/pdf/1406.6247.pdf)

[Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762)

[Multiple Object Recognition with Visual Attention](https://arxiv.org/abs/1412.7755)

[Attention is not all you need anymore](https://arxiv.org/pdf/2308.07661.pdf)

[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=AIiwuClvH6k)

**3.7**

[Large language models surpass human experts in
predicting neuroscience results](https://arxiv.org/pdf/2403.03230.pdf)

**3.8**

[Encoding and decoding in fMRI](https://doi.org/10.1016/j.neuroimage.2010.07.073)

[My favorite math jokes](https://arxiv.org/pdf/2403.01010v1.pdf)

**3.9**

[Memory in humans and deep language models: Linking hypotheses for model augmentation](https://arxiv.org/abs2210.1869)

**3.11**

[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)

[Mathematical introduction to deep learning](https://arxiv.org/pdf/2310.20360.pdf)

**3.12**

[Memory and attention in deep learning](https://arxiv.org/abs/2107.01390)

[World Models and Predictive Coding for Cognitive and Developmental Robotics:
Frontiers and Challenges](https://arxiv.org/pdf/2301.05832.pdf)

[Mastering Memory Tasks with World Models](https://arxiv.org/pdf/2403.04253v1.pdf)

[Mechanism for feature learning in neural networks and backpropagation-free machine learning models](https://www.science.org/doi/10.1126/science.adi5639)

**3.13**

[Brain-inspired intelligent robotics: The intersection of robotics and neuroscience](https://www.science.org/do/10.1126/resource.2375862/full/brain_inspired_robotics_booklet_2016_12_16-1686067069503.pdf)

[Papers mentioned in this article](https://www.jiqizhixin.com/articles/2016-01-04-4)

**3.14**

[One model for the learning of language](https://www.pnas.org/doi/full/10.1073/pnas.2021865119)

**3.15**

[The pitfalls of next-token prediction](https://arxiv.org/pdf/2403.06963.pdf)

**3.16**

[Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

**3.18**

[Neuroscience needs behavior](https://www.cell.com/neuron/pdf/S0896-6273%2816%2931040-6.pdf)

**3.23**

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1)

[Task interference as a neuronal basis for the cost of cognitive flexibility](https://www.biorxiv.org/content/10.1101/2024.03.04.583375v1.full.pdf)

[A Technical Critique of Some Parts of the Free Energy Principle](https://arxiv.org/abs/2001.06408)

**3.24**

[Theories of Error Back-Propagation in the Brain](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9)

[Neurosymbolic AI](https://arxiv.org/abs/2305.00813)

**3.26**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9)

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1.full)

**3.27**

[Reconstructing computational system dynamics from neural data with recurrent neural networks](https://www.nature.com/articles/s41583-023-00740-7)

**3.29**

[A useful guide of how to pronounce common math symbols](http://www.uefap.com/speaking/symbols/symbols.htm)

**3.30**

[A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/pdf/2403.18929.pdf)

**3.31**

[Collective intelligence: A unifying concept for integrating biology across scales and substrates](https://www.nature.com/articles/s42003-024-06037-4)

**4.3**

[An Introduction to Model-Based Cognitive Neuroscience](https://link.springer.com/book/10.1007/978-3-031-45271-0)

[What does it mean to understand a neural network?](https://arxiv.org/abs/1907.06374)

[What is a GPT by 3Blue1Brown](https://www.youtube.com/watch?v=wjZofJX0v4M)

**4.5**

[Nonmonotonic Plasticity: How Memory Retrieval Drives Learning](https://www.sciencedirect.com/science/article/pii/S1364661319301597)

[Single Cortical Neurons as Deep Artificial Neural Networks](https://www.sciencedirect.com/science/article/abs/pii/S0896627321005018)

**4.17**

[The brain&#39;s unique take on algorithms](https://www.nature.com/articles/s41467-023-40535-z)

[Cognition is an emergent property](https://www.sciencedirect.com/science/article/pii/S2352154624000391?via%3Dihub)

**4.18**

[Catalyzing next-generation Artificial Intelligence through NeuroAI](https://www.nature.com/articles/s41467-023-37180-x)

**4.19**

[Toward a formal theory for computing machines made out of whatever physics offers](https://www.nature.com/articles/s41467-023-40533-1)

[Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research](https://www.sciencedirect.com/science/article/pii/S0893608021003683)

**4.22**

[Time, Love, Memory](https://en.wikipedia.org/wiki/Time,_Love,_Memory)

[Thinking About Science](https://www.science.org/doi/10.1126/science.242.4886.1711)

[Reasoning ability is (little more than) working-memory capacity?! - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0160289605800121)

[What Is Life - Wikipedia](https://en.wikipedia.org/wiki/What_Is_Life)

[How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815)

**4.24**

[Empowering Working Memory for Large Language Model Agents](https://arxiv.org/abs/2312.17259)

**4.26**

[Context-dependent computation by recurrent dynamics in prefrontal cortex](https://www.nature.com/articles/nature12742)

[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

**4.29**

[Concurrent maintenance of both veridical and transformed working memory representations within unique coding schemes](https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00173/120839/Concurrent-maintenance-of-both-veridical-and)

**5.1**

[A formal model of capacity limits in working memory - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0749596X06000982)

[The Thermodynamics of Mind: Trends in Cognitive Sciences](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(24)00075-5#%20)

**5.7**

[Bridging Neuroscience and Robotics: Spiking Neural Networks in Action](https://www.mdpi.com/1424-8220/23/21/8880)

[Combined Sensing, Cognition, Learning, and Control for Developing Future Neuro-Robotics Systems: A Survey](https://ieeexplore.ieee.org/document/8634917)

[AI, Robotics &amp; Neuroengineering at Ken Kennedy Institute](https://kenkennedy.rice.edu/research/neuroengineering-ai-robotics)

[Special Issue : Applications of Neural Networks in Robot Control](https://www.mdpi.com/journal/robotics/special_issues/194M9AW732)

[Embodied AI Workshop](https://embodied-ai.org/)

**5.8**

[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)

[A new look at state-space models for neural data, Journal of Computational Neuroscience](https://link.springer.com/article/10.1007/s10827-009-0179-x)

[Latent state-space models for neural decoding](https://ieeexplore.ieee.org/document/6944262)

[State Space Modeling of Neural Spike Train and Behavioral Data - ScienceDirect](https://www.sciencedirect.com/science/article/pii/B9780123750273000065)

[Switching state-space modeling of neural signal dynamics](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011395)

[Robotics and artificial intelligence](https://www.nature.com/immersive/robotics-ai/index.html)

[Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT](https://www.nature.com/articles/s43588-023-00527-x)
[Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT](https://www.nature.com/articles/s43588-023-00527-x)

**5.13**

[Is it a transition or a continuation? From PhD student to Postdoc. - ECR Community](https://ecrcommunity.plos.org/2020/04/03/is-it-a-transition-or-a-continuation-from-phd-student-to-postdoc/)

[Ten Simple Rules for Selecting a Postdoctoral Position](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0020121)

[Transitioning fields between a Ph.D. and postdoc](https://www.science.org/content/article/transitioning-fields-between-phd-and-postdoc)

**5.14**

[The Computational Lens: from Quantum Physics to Neuroscience](https://arxiv.org/abs/2310.20539)

[Integration of cognitive tasks into artificial general intelligence test for large models: iScience](https://www.cell.com/iscience/fulltext/S2589-0042(24)00772-7#secsectitle0050)

[Active Predictive Coding: A Unified Neural Framework for Learning Hierarchical World Models for Perception and Planning](https://arxiv.org/abs/2210.13461)

[From grid cells to place cells: A mathematical model](https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.20244)

[If deep learning is the answer, what is the question?](https://www.nature.com/articles/s41583-020-00395-8)

**5.21**

[The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers](https://arxiv.org/abs/2210.06313v2)

**5.29**

[Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z)

[Neuromorphic dreaming: A pathway to efficient learning in artificial agents](https://arxiv.org/pdf/2405.15616)

**6.2**

[Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)

**6.3**

[Biocomputing with organoid intelligence](https://www.nature.com/articles/s44222-024-00200-6)

[Catalyzing next-generation Artificial Intelligence through NeuroAI](https://www.nature.com/articles/s41467-023-37180-x#peer-review) (Well, this one has been listed above, but never mind)

[Disentangling and Integrating Relational and Sensory Information in Transformer Architectures](https://arxiv.org/abs/2405.16727)

**6.5**

[Empirical influence functions to understand the logic of fine-tuning](https://arxiv.org/abs/2406.00509)

**6.12**

[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)

**6.13**

[A virtual rodent predicts the structure of neural activity across behaviors](https://www.nature.com/articles/s41586-024-07633-4)

[Empirical influence functions to understand the logic of fine-tuning](https://arxiv.org/abs/2406.00509)

[Activation Sparsity: An Insight into the Interpretability of Trained Transformers](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/final-reports/final-report-169721612.pdf)

**6.14**

[Inferences on a multidimensional social hierarchy use a grid-like code](https://www.nature.com/articles/s41593-021-00916-3)

[Grid-like and distance codes for representing word meaning in the human brain](https://www.sciencedirect.com/science/article/pii/S1053811921001531)

[Relating transformers to models and neural representations of the hippocampal formation](https://arxiv.org/abs/2112.04035)

[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
