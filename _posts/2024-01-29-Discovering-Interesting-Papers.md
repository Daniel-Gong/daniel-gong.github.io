---
title: 'Interesting NeuroAI/LLM Cognition/Emobodied AI/miscellaneous papers'
date: 2024-01-29
permalink: /posts/2024/01/readings/
tags:
  - research

---
**1.29.2024**

[Neural tuning and representational geometry](https://www.nature.com/articles/s41583-021-00502-3), *Nature Reviews Neuroscience*, 2021
Nikolaus Kriegeskorte & Xue-Xin Wei

**1.30.2024**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9), *Nature Machine Intelligence*, 2023
Jascha Achterberg et al.

**2.1.2024**

[Transformer as a hippocampal memory consolidation
model based on NMDAR-inspired nonlinearity](https://openreview.net/pdf?id=vKpVJxplmB), *NeurIPS*, 2023

**2.5.2024**

[Brains and algorithms partially converge in natural language processing](https://www.nature.com/articles/s42003-022-03036-1), *Communications Biology*, 2022

**2.7.2024**

[No Coincidence, George: Capacity-Limits as the Curse of Compositionality](https://osf.io/preprints/psyarxiv/cjuxb), PsyArXiv, 2022

**2.12.2024**

[Structural constraints on the emergence of oscillations in multi-population neural networks](https://elifesciences.org/reviewed-preprints/88777), eLife, 2024

[Oscillatory neural networks](https://www.youtube.com/watch?v=RNr7-gHxB4g), YouTube

**2.14**

[Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons](https://link.springer.com/article/10.1023/a:1008925309027)

**2.16**

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

[Mechanisms of Gamma Oscillations](https://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150444)

**2.17**

[A call for embodied AI](https://arxiv.org/abs/2402.03824)

**2.18**

[Circular and unified analysis in network neuroscience](https://elifesciences.org/articles/79559)

**2.20-2.27**

I was at AAAI 2024 for nearly a week. I learned a lot and will share some papers I came across from talks/posters at the conference.

[On the Paradox of Learning to Reason from Data](https://arxiv.org/pdf/2205.11502.pdf)

[CRAB: Assessing the Strength of Causal Relationships
Between Real-World Events](https://arxiv.org/pdf/2311.04284.pdf)

[Passive learning of active causal strategies in agents
and language models](https://arxiv.org/pdf/2305.16183.pdf)

[SPARTQA: A Textual Question Answering Benchmark
for Spatial Reasoning](https://arxiv.org/pdf/2104.05832.pdf)

[Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

**3.1**

[Three aspects of representation in neuroscience](https://www.sciencedirect.com/science/article/pii/S1364661322002108)

[Redefining &#34;Hallucination&#34; in LLMs: Towards a psychology-informed framework for mitigating misinformation](https://arxiv.org/abs/2402.01769v1)

[Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

**3.2**

[Neural Turing Machines](https://arxiv.org/abs/1410.5401)

[A Critical Review of Causal Reasoning Benchmarks for Large Language Models](https://openreview.net/pdf?id=mRwgczYZFJ)

**3.3**

[Recurrent Models of Visual Attention](https://arxiv.org/pdf/1406.6247.pdf)

[Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762)

[Multiple Object Recognition with Visual Attention](https://arxiv.org/abs/1412.7755)

[Attention is not all you need anymore](https://arxiv.org/pdf/2308.07661.pdf)

[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=AIiwuClvH6k)

**3.7**

[Large language models surpass human experts in
predicting neuroscience results](https://arxiv.org/pdf/2403.03230.pdf)

**3.8**

[Encoding and decoding in fMRI](https://doi.org/10.1016/j.neuroimage.2010.07.073)

[My favorite math jokes](https://arxiv.org/pdf/2403.01010v1.pdf)

**3.9**

[Memory in humans and deep language models: Linking hypotheses for model augmentation](https://arxiv.org/abs2210.1869)

**3.11**

[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)

[Mathematical introduction to deep learning](https://arxiv.org/pdf/2310.20360.pdf)

**3.12**

[Memory and attention in deep learning](https://arxiv.org/abs/2107.01390)

[World Models and Predictive Coding for Cognitive and Developmental Robotics:
Frontiers and Challenges](https://arxiv.org/pdf/2301.05832.pdf)

[Mastering Memory Tasks with World Models](https://arxiv.org/pdf/2403.04253v1.pdf)

[Mechanism for feature learning in neural networks and backpropagation-free machine learning models](https://www.science.org/doi/10.1126/science.adi5639)

**3.13**

[Brain-inspired intelligent robotics: The intersection of robotics and neuroscience](https://www.science.org/do/10.1126/resource.2375862/full/brain_inspired_robotics_booklet_2016_12_16-1686067069503.pdf)

[Papers mentioned in this article](https://www.jiqizhixin.com/articles/2016-01-04-4)

**3.14**

[One model for the learning of language](https://www.pnas.org/doi/full/10.1073/pnas.2021865119)

**3.15**

[The pitfalls of next-token prediction](https://arxiv.org/pdf/2403.06963.pdf)

**3.16**

[Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)

[Using large language models to study human memory for meaningful narratives](https://www.biorxiv.org/content/10.1101/2023.11.03.565484v1.full)

**3.18**

[Neuroscience needs behavior](https://www.cell.com/neuron/pdf/S0896-6273%2816%2931040-6.pdf)

**3.23**

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1)

[Task interference as a neuronal basis for the cost of cognitive flexibility](https://www.biorxiv.org/content/10.1101/2024.03.04.583375v1.full.pdf)

[A Technical Critique of Some Parts of the Free Energy Principle](https://arxiv.org/abs/2001.06408)

**3.24**

[Theories of Error Back-Propagation in the Brain](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9)

[Neurosymbolic AI](https://arxiv.org/abs/2305.00813)

**3.26**

[Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings](https://www.nature.com/articles/s42256-023-00748-9)

[Traveling waves shape neural population dynamics enabling predictions and internal model updating](https://www.biorxiv.org/content/10.1101/2024.01.09.574848v1.full)

**3.27**

[Reconstructing computational system dynamics from neural data with recurrent neural networks](https://www.nature.com/articles/s41583-023-00740-7)

**3.29**

[A useful guide of how to pronounce common math symbols](http://www.uefap.com/speaking/symbols/symbols.htm)

**3.30**

[A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/pdf/2403.18929.pdf)

**3.31**

[Collective intelligence: A unifying concept for integrating biology across scales and substrates](https://www.nature.com/articles/s42003-024-06037-4)

**4.3**

[An Introduction to Model-Based Cognitive Neuroscience](https://link.springer.com/book/10.1007/978-3-031-45271-0)

[What does it mean to understand a neural network?](https://arxiv.org/abs/1907.06374)

[What is a GPT by 3Blue1Brown](https://www.youtube.com/watch?v=wjZofJX0v4M)

**4.5**

[Nonmonotonic Plasticity: How Memory Retrieval Drives Learning](https://www.sciencedirect.com/science/article/pii/S1364661319301597)

[Single Cortical Neurons as Deep Artificial Neural Networks](https://www.sciencedirect.com/science/article/abs/pii/S0896627321005018)

**4.17**

[The brain&#39;s unique take on algorithms](https://www.nature.com/articles/s41467-023-40535-z)

[Cognition is an emergent property](https://www.sciencedirect.com/science/article/pii/S2352154624000391?via%3Dihub)

**4.18**

[Catalyzing next-generation Artificial Intelligence through NeuroAI](https://www.nature.com/articles/s41467-023-37180-x)

**4.19**

[Toward a formal theory for computing machines made out of whatever physics offers](https://www.nature.com/articles/s41467-023-40533-1)

[Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research](https://www.sciencedirect.com/science/article/pii/S0893608021003683)

**4.22**

[Time, Love, Memory](https://en.wikipedia.org/wiki/Time,_Love,_Memory)

[Thinking About Science](https://www.science.org/doi/10.1126/science.242.4886.1711)

[Reasoning ability is (little more than) working-memory capacity?! - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0160289605800121)

[What Is Life - Wikipedia](https://en.wikipedia.org/wiki/What_Is_Life)

[How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815)

**4.24**

[Empowering Working Memory for Large Language Model Agents](https://arxiv.org/abs/2312.17259)

**4.26**

[Context-dependent computation by recurrent dynamics in prefrontal cortex](https://www.nature.com/articles/nature12742)

[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

**4.29**

[Concurrent maintenance of both veridical and transformed working memory representations within unique coding schemes](https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00173/120839/Concurrent-maintenance-of-both-veridical-and)

**5.1**

[A formal model of capacity limits in working memory - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0749596X06000982)

[The Thermodynamics of Mind: Trends in Cognitive Sciences](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(24)00075-5#%20)

**5.7**

[Bridging Neuroscience and Robotics: Spiking Neural Networks in Action](https://www.mdpi.com/1424-8220/23/21/8880)

[Combined Sensing, Cognition, Learning, and Control for Developing Future Neuro-Robotics Systems: A Survey](https://ieeexplore.ieee.org/document/8634917)

[AI, Robotics &amp; Neuroengineering at Ken Kennedy Institute](https://kenkennedy.rice.edu/research/neuroengineering-ai-robotics)

[Special Issue : Applications of Neural Networks in Robot Control](https://www.mdpi.com/journal/robotics/special_issues/194M9AW732)

[Embodied AI Workshop](https://embodied-ai.org/)

**5.8**

[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)

[A new look at state-space models for neural data, Journal of Computational Neuroscience](https://link.springer.com/article/10.1007/s10827-009-0179-x)

[Latent state-space models for neural decoding](https://ieeexplore.ieee.org/document/6944262)

[State Space Modeling of Neural Spike Train and Behavioral Data - ScienceDirect](https://www.sciencedirect.com/science/article/pii/B9780123750273000065)

[Switching state-space modeling of neural signal dynamics](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011395)

[Robotics and artificial intelligence](https://www.nature.com/immersive/robotics-ai/index.html)

[Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT](https://www.nature.com/articles/s43588-023-00527-x)

**5.13**

[Is it a transition or a continuation? From PhD student to Postdoc. - ECR Community](https://ecrcommunity.plos.org/2020/04/03/is-it-a-transition-or-a-continuation-from-phd-student-to-postdoc/)

[Ten Simple Rules for Selecting a Postdoctoral Position](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0020121)

[Transitioning fields between a Ph.D. and postdoc](https://www.science.org/content/article/transitioning-fields-between-phd-and-postdoc)

**5.14**

[The Computational Lens: from Quantum Physics to Neuroscience](https://arxiv.org/abs/2310.20539)

[Integration of cognitive tasks into artificial general intelligence test for large models: iScience](https://www.cell.com/iscience/fulltext/S2589-0042(24)00772-7#secsectitle0050)

[Active Predictive Coding: A Unified Neural Framework for Learning Hierarchical World Models for Perception and Planning](https://arxiv.org/abs/2210.13461)

[From grid cells to place cells: A mathematical model](https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.20244)

[If deep learning is the answer, what is the question?](https://www.nature.com/articles/s41583-020-00395-8)

**5.21**

[The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers](https://arxiv.org/abs/2210.06313v2)

**5.29**

[Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z)

[Neuromorphic dreaming: A pathway to efficient learning in artificial agents](https://arxiv.org/pdf/2405.15616)

**6.2**

[Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)

**6.3**

[Biocomputing with organoid intelligence](https://www.nature.com/articles/s44222-024-00200-6)

[Catalyzing next-generation Artificial Intelligence through NeuroAI](https://www.nature.com/articles/s41467-023-37180-x#peer-review) (Well, this one has been listed above, but never mind)

[Disentangling and Integrating Relational and Sensory Information in Transformer Architectures](https://arxiv.org/abs/2405.16727)

**6.5**

[Empirical influence functions to understand the logic of fine-tuning](https://arxiv.org/abs/2406.00509)

**6.12**

[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)

**6.13**

[A virtual rodent predicts the structure of neural activity across behaviors](https://www.nature.com/articles/s41586-024-07633-4)

[Empirical influence functions to understand the logic of fine-tuning](https://arxiv.org/abs/2406.00509)

[Activation Sparsity: An Insight into the Interpretability of Trained Transformers](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/final-reports/final-report-169721612.pdf)

**6.14**

[Inferences on a multidimensional social hierarchy use a grid-like code](https://www.nature.com/articles/s41593-021-00916-3)

[Grid-like and distance codes for representing word meaning in the human brain](https://www.sciencedirect.com/science/article/pii/S1053811921001531)

[Relating transformers to models and neural representations of the hippocampal formation](https://arxiv.org/abs/2112.04035)

[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)

[Organizing conceptual knowledge in humans with a gridlike code](https://www.science.org/doi/10.1126/science.aaf0941)

[The coming decade of digital brain research: A vision for neuroscience at the intersection of technology and computing](https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00137/120391/The-coming-decade-of-digital-brain-research-A)

**6.18**

[Thousand Brains Project](https://www.numenta.com/thousand-brains-project/)

[千脑智能理论：开启创造机器智能的路线图](https://swarma.org/?p=37892)

**6.24**

[Oxford ML School](https://www.oxfordml.school/)

[Oxford LLMs](https://www.llmsforsocialsciene.dev/)

[Large Language Models for Mathematicians](https://arxiv.org/pdf/2312.04556)

**6.25**

[Language is primarily a tool for communication rather than thought](https://www.nature.com/articles/s41586-024-07522-w)

[Representation learning for neural population activity with Neural Data Transformers](https://arxiv.org/abs/2108.01210)

[Towards a Foundation Model of the Mouse Visual Cortex](https://www.biorxiv.org/content/10.1101/2023.03.21.533548v2.full)

[Statistical mechanics of Bayesian inference and learning in neural networks](https://dash.harvard.edu/handle/1/37378715)

[Jascha Achterberg - NeuroAI](https://www.jachterberg.com/neuroai)

**6.26**

[A Bayesian account of learning and generalising representations in the brain - ORA - Oxford University Research Archive](https://ora.ox.ac.uk/objects/uuid:2b513340-9558-41dd-8533-0f250df98c66)

[Detecting hallucinations in large language models using semantic entropy](https://www.nature.com/articles/s41586-024-07421-0)

[Fine-tuning can cripple your foundation model; preserving features may be the solution](https://arxiv.org/abs/2308.13320)

**7.12**

[Working Memory Load Modulates Neuronal Coupling](https://academic.oup.com/cercor/article/29/4/1670/4955775)

[In vivo ephaptic coupling allows memory network formation](https://academic.oup.com/cercor/article/33/17/9877/7220593)

**7.16**

[Cognitive computational neuroscience](https://www.nature.com/articles/s41593-018-0210-5)

[Heavy-tailed neuronal connectivity arises from Hebbian self-organization](https://www.nature.com/articles/s41567-023-02332-9)

[INSTRUCTION-TUNING ALIGNS LLMS TO THE HUMAN BRAIN](https://arxiv.org/pdf/2312.00575)

[The debate over understanding in AI’s large language models](https://www.pnas.org/doi/10.1073/pnas.2215907120)

**7.18**

[Shared functional specialization in transformer-based language models and the human brain](https://www.nature.com/articles/s41467-024-49173-5)

[On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)

**7.19**

[The expanding horizons of network neuroscience: From description to prediction and control - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1053811922003743?via%3Dihub)

[Modular Brain Networks](https://www.annualreviews.org/content/journals/10.1146/annurev-psych-122414-033634)

**7.31**

[Organic electrochemical neurons and synapses with ion mediated spiking](https://www.nature.com/articles/s41467-022-28483-6)

**8.2**

[Stephen Wolfram: A New Kind of Science](https://www.wolframscience.com/nks/)

**8.3**

[Do Language Models Have a Critical Period for Language Acquisition?](https://arxiv.org/abs/2407.19325#)

**8.5**

[Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs](https://arxiv.org/abs/2408.00114)

**8.7**

[From Analog to Digital Computing: Is Homo sapiens’ Brain on Its Way to Become a Turing Machine?](https://www.frontiersin.org/journals/ecology-and-evolution/articles/10.3389/fevo.2022.796413/full)

**8.13**

[The brain and its time: intrinsic neural timescales are key for input processing](https://www.nature.com/articles/s42003-021-02483-6)

**8.28**

[Neural circuits as computational dynamical systems](https://www.sciencedirect.com/science/article/pii/S0959438814000166)

**9.9**

[Unsupervised neural network models of the ventral visual stream](https://www.pnas.org/doi/full/10.1073/pnas.2014196118)

[Emotional Intelligence of Large Language Models](https://arxiv.org/abs/2307.09042)

[No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit](https://openreview.net/pdf?id=mxi1xKzNFrb)

[CEBRA: Learnable latent embeddings for joint behavioral and neural analysis](https://arxiv.org/pdf/2204.00673)

[DevBench: A multimodal developmental benchmark for language learning](https://arxiv.org/abs/2406.10215)

[Running cognitive evaluations on large language models: The do&#39;s and the don&#39;ts](https://arxiv.org/abs/2312.01276)

[Induction heads - illustrated — LessWrong](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)

[Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks](https://arxiv.org/abs/2210.00400)

[Abstract representations emerge in human hippocampal neurons during inference](https://www.nature.com/articles/s41586-024-07799-x)

[Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes](https://arxiv.org/abs/2312.08519)

[Lecture Notes on Infinite-Width Limits of Neural Networks](https://pehlevan.seas.harvard.edu/sites/projects.iq.harvard.edu/files/pehlevan/files/princeton_lecture_notes.pdf)

[Scaling and renormalization in high-dimensional regression](https://arxiv.org/pdf/2405.00592)

[Curriculum Learning with Infant Egocentric Videos](https://openreview.net/pdf?id=zkfyOkBVpz)

[In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)

[Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0893608021003683)

[Reasoning ability is (little more than) working-memory capacity?!](https://www.sciencedirect.com/science/article/pii/S0160289605800121)

[A formal model of capacity limits in working memory](https://www.sciencedirect.com/science/article/pii/S0749596X06000982#fig6)

[Prefrontal cortex as a meta-reinforcement learning system](https://www.nature.com/articles/s41593-018-0147-8)

[Scaffolding cooperation in human groups with deep reinforcement learning](https://www.nature.com/articles/s41562-023-01686-7)

[Sequential Memory with Temporal Predictive Coding](https://arxiv.org/pdf/2305.11982)

[COGNITIVE MODELING OF SEMANTIC FLUENCY USING TRANSFORMERS](https://arxiv.org/pdf/2208.09719)

[Predictive Coding: a Theoretical and Experimental Review](https://arxiv.org/abs/2107.12979)

[Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes](https://arxiv.org/pdf/2305.11772)

[Toward the Emergence of Intelligent Control: Episodic Generalization and Optimization](https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00143/121081/Toward-the-Emergence-of-Intelligent-Control)

[Machine learning Notation](https://nthu-datalab.github.io/ml/slides/Notation.pdf)

[Accelerating generative models and nonconvex optimisation](https://www.turing.ac.uk/research/theory-and-method-challenge-fortnights/accelerating-generative-models-and-nonconvex)

[Representation and computation in visual working memory](https://www.nature.com/articles/s41562-024-01871-2?fromPaywallRec=false)

[Nonlinear difference equations](https://math.dartmouth.edu/opencalc2/dcsbook/c1pdf/sec15.pdf)

[Dynamical Systems Approaches to Cognition](https://dynamicfieldtheory.org/upload/file/1678900855_6d313f335a2edf61ad6d/Schoner2022_proofs.pdf)

[Attention Mechanisms and Their Applications to Complex Systems - PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7996841/)

[Learning differential equations](https://www.sfu.ca/math-coursenotes/Math%20158%20Course%20Notes/chap_DifferentialEquations.html)

[Context-dependent computation by recurrent dynamics in prefrontal cortex](https://www.nature.com/articles/nature12742)

[Evidence of a predictive coding hierarchy in the human brain listening to speech](https://www.nature.com/articles/s41562-022-01516-2)

[Using higher-order Markov models to reveal flow-based communities in networks](https://www.nature.com/articles/srep23194#Fig1)

[he Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)

[The neuron as a direct data-driven controller](https://www.pnas.org/doi/10.1073/pnas.2311893121)

[Seminar course: Bridging Language in Machines and Language in the Brain](https://bridge-ai-neuro.github.io/)

[Manifolds: A Gentle Introduction](https://bjlkeng.io/posts/manifolds/)

[Dimension Reduction using Isomap](https://medium.com/data-science-in-your-pocket/dimension-reduction-using-isomap-72ead0411de)

[A simple weight decay can...](https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf)

[Investigating Neuron Ablation in Attention Heads: The Case for Peak Activation Centering](https://arxiv.org/pdf/2408.17322)

[Empirical influence functions to understand the logic of fine-tuning](https://arxiv.org/abs/2406.00509)

**9.14**

[The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)

**9.15**

[How Attention works in Deep Learning: understanding the attention mechanism in sequence models](https://theaisummer.com/attention/)

[Explainable AI: Visualizing Attention in Transformers - Comet](https://www.comet.com/site/blog/explainable-ai-for-transformers/)

[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)

[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

[Transformers: a Primer](https://www.columbia.edu/~jsl2239/transformers.html)

[Code repo: Word-level Language Modeling using RNN and Transformer](https://github.com/pytorch/examples/tree/main/word_language_model)

[Code repo: Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity](https://github.com/kdkyum/transformer_memory_consolidation)

[Code repo: The Tolman-Eichenbaum Machine](https://github.com/djcrw/generalising-structural-knowledge)

[Adaptive chunking improves effective working memory capacity in a prefrontal cortex and basal ganglia circuit](https://www.biorxiv.org/content/10.1101/2024.03.24.586455v1.full)

[Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://ieeexplore.ieee.org/abstract/document/10136140)

[Analogous computations in working memory input, output and motor gating: Electrophysiological and computational modeling evidence](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008971)

[Exposing Attention Glitches with Flip-Flop Language Modeling](https://proceedings.neurips.cc/paper_files/paper/2023/hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html)

[Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks](https://direct.mit.edu/neco/article-abstract/25/3/626/7854/Opening-the-Black-Box-Low-Dimensional-Dynamics-in?redirectedFrom=fulltext)

[Context-dependent computation by recurrent dynamics in prefrontal cortex](https://www.nature.com/articles/nature12742)

[A survey of transformers](https://www.sciencedirect.com/science/article/pii/S2666651022000146#b137)

[Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion](https://www.nature.com/articles/s42256-022-00498-0)

[Population codes enable learning from few examples by shaping inductive bias](https://elifesciences.org/articles/78606#content)

[What is In-context Learning, and how does it work: The Beginner’s Guide](https://www.lakera.ai/blog/what-is-in-context-learning)

[The Curse of Dimensionality](https://medium.com/@m.chellaa/the-curse-of-dimensionality-974e60b09835)

[A generative model of memory construction and consolidation](https://www.nature.com/articles/s41562-023-01799-z)

[The Neurobiology of Semantic Memory](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3350748/)

[The precision of visual working memory is set by allocation of a shared resource](https://jov.arvojournals.org/article.aspx?articleid=2122354)

[The capacity of visual working memory for features and conjunctions](https://www.nature.com/articles/36846)

[Timescales of learning in prefrontal cortex](https://www.nature.com/articles/s41583-024-00836-8)

[The Distributed Nature of Working Memory](https://www.sciencedirect.com/science/article/pii/S1364661316302170)

[Geometry of neural computation unifies working memory and planning](https://www.pnas.org/doi/full/10.1073/pnas.2115610119)

[Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks](https://arxiv.org/abs/2402.08211)

[Wide Attention Is The Way Forward For Transformers?](https://arxiv.org/abs/2210.00640)

[The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers](https://arxiv.org/abs/2108.12284)

[Natural constraints explain working memory capacity limitations in sensory-cognitive models](https://www.biorxiv.org/content/10.1101/2023.03.30.534982v1.full)

[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

[The Depth-to-Width Interplay in Self-Attention](https://arxiv.org/pdf/2006.12467)

[A mathematical perspective on Transformers](https://arxiv.org/abs/2312.10794)

[TOWARDS SMALLER, FASTER DECODER-ONLY TRANSFORMERS: ARCHITECTURAL VARIANTS AND THEIR IMPLICATIONS](https://arxiv.org/pdf/2404.14462)

[Upper and lower memory capacity bounds of transformers for next-token prediction](https://arxiv.org/abs/2405.13718)

[How Powerful are Decoder-Only Transformer Neural Models?](https://arxiv.org/abs/2305.17026)

[Mastering Decoder-Only Transformer: A Comprehensive Guide](https://www.analyticsvidhya.com/blog/2024/04/mastering-decoder-only-transformer-a-comprehensive-guide/#:~:text=Typically%2C%20scaling%20up%20a%20decoder,heads%20within%20each%20attention%20layer.)

[How should the architecture of a transformer be scaled? : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/17g1il0/dr_how_should_the_architecture_of_a_transformer/)

[Code repo: Transformer_walkthrough](https://github.com/markriedl/transformer-walkthrough/blob/main/Transformer_walkthrough.ipynb)

[PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks](https://www.eneuro.org/content/8/1/ENEURO.0427-20.2020)

[Self-backpropagation of synaptic modifications elevates the efficiency of spiking and artificial neural networks](https://www.science.org/doi/10.1126/sciadv.abh0146)

**9.17**

[STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making](https://arxiv.org/abs/2405.16376)

[Automated construction of cognitive maps with visual predictive coding](https://www.nature.com/articles/s42256-024-00863-1)

[Schrodinger&#39;s Memory: Large Language Models](https://arxiv.org/abs/2409.10482)

[From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures](https://arxiv.org/abs/2407.01548)

[Neuroscience + Artificial Intelligence = NeuroAI](https://zuckermaninstitute.columbia.edu/neuroscience-artificial-intelligence-neuroai)

[Transformer-based Working Memory for Multiagent Reinforcement Learning with Action Parsing](https://proceedings.neurips.cc/paper_files/paper/2022/hash/e1cf57f1e104c6c05e31894c15a65e99-Abstract-Conference.html)

[[2402.12875] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875)

[[2103.03404] Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/abs/2103.03404)

**9.18**

[InversionView: A General-Purpose Method for Reading Information from Neural Activations](https://arxiv.org/abs/2405.17653)

[Divergent recruitment of developmentally defined neuronal ensembles supports memory dynamics](https://www.science.org/doi/10.1126/science.adk0997)

[Theoretical Limitations of Self-Attention in Neural Sequence Models](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00306/43545/Theoretical-Limitations-of-Self-Attention-in)

[TransformerFAM: Feedback attention is working memory](https://arxiv.org/abs/2404.09173)

[A resource-rational model of human processing of recursive linguistic structure](https://www.pnas.org/doi/10.1073/pnas.2122602119)

**9.19**

[Empirical Capacity Model for Self-Attention Neural Networks](https://arxiv.org/pdf/2407.15425)

[Self-attention Does Not Need $O(n^2)$ Memory](https://arxiv.org/abs/2112.05682)
